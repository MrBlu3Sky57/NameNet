\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx} % Required for inserting images

\title{Back propagation for Deep Feedforward Neural Networks}
\author{Aaron Avram}
\date{June 10 2025}

\begin{document}

\maketitle

\section*{Introduction}
In this write up I will go through the derivation of the back propagation algorithm
for deep feedforward neural networks and discuss how I will implement it using
NumPy.

\section*{Set up}
We are working with a classic neural net set up where we have $n + 1$ layers
$\{x^{(0)}, \ldots, x^{(n)}\}$ with the $i-th$ layer having $l_i$ neurons. We then define a weight matrix and bias vector for each of the $n$ non-input neurons.
$W^{(i)}_{jk}$ is the weight of the $k$-th neuron in the $i-1$th on the $j$th neuron
in the $i$th layer. And, $b^{(i)}_j$ is the bias associated with the $j$th neuron in the 
$i$th layer. Denote the activation function by $\sigma$ (which could be $\tanh$ for example). Note that when
$\sigma$ is applied to a vector we do so element wise. Now we can represent the forward pass between each layer succinctly where
\begin{align*}
    x^{(i)} = \sigma(W^{(i)}x^{(i-1)} + b^{(i)})
\end{align*}
It will be convenient later to denote $z^{(i)} := W^{(i)}x^{(i-1)} + b^{(i)}$, so
that $x^{(i)} = \sigma(z^{(i)})$. Note, that there may be some transformation
on the output layer so that it represents a probability distribution (I.e. applying the softmax),
but this is trivial in the context of back propagation calculations so it won't be included here.


Now when training this model on some input, we optimize its performance
by minimizing some loss function, which measures some numerical representation
of how well the model performs on the training data. Formally: $L: \mathbb{R}^{(l_n)} \to \mathbb{R}$. In the
following calculations we will take gradients and derivatives of the loss function, and so we will assume
that all of the mathematical functions involved are differentiable, which is practically always the case. Now,
to train our model we want to find a method for computing the gradient of the weights and biases with respect to our loss function
then perform gradient descent. Back propagation is such a method.

\section*{The Plan}
The back propagation algorithm is essentially a fancy application of
the chain rule from calculus. Namely the chain rule states that for a composition
of functions $f, g, h$ we have:
\begin{align*}
   \frac{df(g(h))} {dx} = \frac{df}{dg}\frac{dg}{dh}\frac{dh}{dx}
\end{align*}
In the multivariable case we apply this to partial derivatives for each component of input
and output respectively.


Clearly our neural network is just a very large and complicated function and to get the gradients
of the weights and biases, we can apply the above concept. This looks similar to dynamic programming
where we are using memoization by caching the gradients of the preceeding layers as we differentiate
through the network.

\section*{Back Prop}
Now, we will build up our gradients recursively working backwards from the output layer.


Since our loss function has scalar output, the gradient of the output layer
with respect to the loss is straight forward:
\begin{align*}
    \nabla_{x^{(n)}}(L) = \begin{bmatrix}
        \frac{\partial L}{\partial x^{(n)}_1} \\ \ldots \\ \frac{\partial L}{\partial x^{(n)}_{l_n}}
    \end{bmatrix}
\end{align*}
Then for any layer $x^{(i)}$ we can consider the Jacobian of $z^{(i)}$ with
respect to the output. However since $z^{(i)}_j$ only effects $x^{(i)}_j$ the
Jacobian will be diagonal, so we can just write it as a gradient vector (abusing notation
slightly).
\begin{align*}
        \nabla_{z^{(i)}}(x^{(i)}) = \begin{bmatrix}
        \frac{\partial x^{(i)}_1}{\partial z^{(i)}_1} \\ \ldots \\ \frac{\partial x^{(i)}_{l_i}}{\partial z^{(i)}_{l_i}}
    \end{bmatrix}
\end{align*}
Additionally we know that $x^{(i)} = \sigma(z^{(i)})$, so $\nabla_{z^{(i)}}(x^{(i)}) = \sigma'(z^{(i)})$. 
Next we want to conisder the Jacobian of $x^{(i-1)}$ with respect to $z^{(i)}$, but notice that
$z^{(i)} = W^{(i)}x^{(i-1)} + b^{(i)}$ so $\frac{\partial z^{(i)}_j}{\partial x^{(i-1)}_k} = W^{(i)}_{jk}$. Thus:
\begin{align*}
    J(z^{(i)})(x^{(i-1)}) &= \begin{bmatrix}
        \frac{\partial z^{(i)}_1}{\partial x^{(i-1)}_1} & \ldots  & \frac{\partial z^{(i)}_{l_i}}{\partial x^{(i-1)}_1} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial z^{(i)}_1}{\partial x^{(i-1)}_{l_{i-1}}} & \ldots  & \frac{\partial z^{(i)}_{l_i}}{\partial x^{(i-1)}_{l_{i-1}}} \\
    \end{bmatrix}
    \\ \\
    &= (W^{(i)})^T
\end{align*}
Abusing notation again we will associate this with a gradient vector $\nabla_{x^{(i-1)}}(z^{(i)})$. Now we can combine the following results to find
the Jacobian of $x^{(i-1)}$ with respect to $x^{(i)}$. Observe that $\frac{\partial x^{(i)}_j}{\partial x^{(i-1)}_k} = 
\frac{\partial x^{(i)}_j}{\partial z^{(i)}_j}\frac{\partial z^{(i)}_j}{\partial x^{(i-1)}_k}$ and we know both of these values.
Thus, The Jacobian of $x^{(i-1)}$ with respect to $x^{(i)}$ is just the  Jacobian of $x^{(i-1)}$ with respect to $z^{(i)}$ with each row
scaled by $\nabla_{z^{(i)}}(x^{(i)})$. So:
\begin{align*}
    \nabla_{x^{(i-1)}}(x^{(i)}) = \nabla_{x^{i-1}}(z^{(i)}) \odot \nabla_{z^{(i)}}(x^{(i)})^T
\end{align*}
Note we assume that the row vector in the equation is broadcasted by copying the rows on top of each other.



With this we have a way of recursively calculating the gradient of each layer
with respect to the loss function. The final step is to now compute the gradients with
respect to the weights and biases of each layer and store them.



First consider $z^{(i)}$ and $W^{(i)}$, note that we can compute the Jacobian of the
flattened version of $W^{(i)}$ with respect to $z^{(i)}$ but if we observe that
the $j$th component of $z^{(i)}$ is only affected by the $j$th row of $W^{(i)}$
we can construct our jacobian matrix as containing the partial derivatives of each component of the
$j$th row of $W^{(i)}$ with respect to the $j$th component of $z^{(i)}$ as its columns. Now since
$z^{(i)} = W^{(i)}x^{(i-1)} + b^{(i)}$, $\frac{\partial z^{(i)}_j}{\partial W^{i}_{jk}} = x^{(i-1)}_k$. Thus:
\begin{align*}
    J(z^{(i)})(W^{(i)}) &= \begin{bmatrix}
        \frac{\partial z^{(i)}_1}{\partial W^{(i)}_{11}} & \ldots  & \frac{\partial z^{(i)}_{l_i}}{\partial W^{(i)}_{l_i1}} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial z^{(i)}_1}{\partial W^{(i)}_{1l_{i-1}}} & \ldots  & \frac{\partial z^{(i)}_{l_i}}{\partial W^{(i)}_{l_il_{i-1}}} \\
    \end{bmatrix} \\ \\
    &= \begin{bmatrix}
        x^{(i-1)}_1 & \ldots  & x^{(i-1)}_1 \\
        \vdots & \ddots & \vdots \\
       x^{(i-1)}_{l_{i-1}} & \ldots  & x^{(i-1)}_{l_{i-1}} \\
    \end{bmatrix}
\end{align*}
Thus we can succinctly write the combined gradient as a product:
\begin{align*}
    \nabla_{W^{(i)}}(x^{(i)}) &=  J(z^{(i)})(W^{i}) \odot \nabla_{z^{(i)}}(x^{(i)})\\
    &= x^{(i-1)}\nabla_{z^{(i)}}(x^{(i)})^T
\end{align*}
 
Next for the $b^{(i)}$ term since it is just a constant term and only its $j$th component affects
the $j$-th component of $z^{(i)}$, $\nabla_{b^{(i)}}(z^{(i)}) = 1_{l_i}$ and
$\nabla_{b^{(i)}}(x^{(i)}) = \nabla_{z^{(i)}}(x^{(i)})$. With this, we have
fully detailed the recursive algorithm steps.

\section*{Implementation}
Since I am building this model in pure NumPy and Python, I want to leverage
NumPy BLAS calls as much as possible, thus I will opt for storing the necessary gradients
as arrays and using vectorized operations to compute each back prop step. Thus, I won't pass in indiviudal
examples during training but instead I will pass in an array of them. The backprop step will look similar to what has
been derived above, however I will store a matrix for each layer gradient. The weight and bias gradients
will be the summed across training examples which happens naturally with matrix multiplication of the layer arrays.
I.e. notice that, if the input batch has $m$ training examples with x now having its $i$th row
being the $i$th training example. With this setup there is an elegant way
to express the jacobian of the loss with respect to the last unactivated layer if the output
activation is a softmax, and the loss is cross entropy:
\begin{align*}
    J(L)(z^{(n)}) = x^{(n)} - y_{\text{onehot}}
\end{align*}
And we can express the gradients of the weights by considering the same
construction as in the single example case, but with the gradients per example
summed. Thus, for $i < n$:
\begin{align*}
    J(L)(W^{(i)})_{jk} &= \sum_{p=1}^{m}\frac{L}{\partial W^{(i)}_{kj}} \\
    &= \sum_{p=1}^{m}\frac{L}{\partial z^{(i)}_{pk}}\frac{\partial z^{(i)}_{pk}}{\partial W^{(i)}_{kj}}
\end{align*}
Plugging in values:
\begin{align*}
    J(L)(W^{(i)})_{jk} &= \sum_{p=1}^{m}\frac{L}{\partial z^{(i)}_{pk}}x_{pj}^{(i-1)}
\end{align*}
If we let G be the $m, l_i$ matrix of the gradients of each unactivated neuron in the $i$th layer
we can write this as a matrix multiplication: $J(L)(W^{(i)}) = G^TX^{i-1}$.


Then, $J(L)(B^{(i)})$ is just a sum across training examples for the previous unactivated layer grads. 


Next to find $J(L)(X^{(i-1)})$ consider:
\begin{align*}
    J(L)(X^{(i-1)})_{jk} &= \frac{\partial L}{\partial x^{(i-1)}_jk} \\
    &= \sum_{p = 0}^{l_i}\frac{\partial L}{\partial z^{(i)}_{jp}}\frac{\partial z^{(i)}_{jp}}{x^{(i-1)}_{jk}} \\
    &= \sum_{p = 0}^{l_i}\frac{\partial L}{\partial z^{(i)}_{jp}}W^{(i)}_{pk}
\end{align*}
If we let the matrix G now represent the $m, l_i$ matrix containing the gradients
of the unactivated $i$th we have:
\begin{align*}
    J(L)(X^{(i-1)}) &= GW^{(i)}
\end{align*}
\end{document}