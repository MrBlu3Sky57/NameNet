{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cfeb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import MLP, grad_descent, parse_txt, char_tokenize, itos, stoi\n",
    "from net.util import tanh, dtanh, cross_entropy, SEED\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae5c41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BLOCK_SIZE = 4\n",
    "FEATURES = 64\n",
    "VOCAB_SIZE = 27\n",
    "size = (BLOCK_SIZE * FEATURES, 256, VOCAB_SIZE)\n",
    "LR = 0.1\n",
    "ALPHA = 1e-4\n",
    "STEPS = 200000\n",
    "BATCH_SIZE = 30\n",
    "TEMPERATURE = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487f7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name data\n",
    "names = parse_txt(\"../data/names.txt\")\n",
    "xs, ys, vocab = char_tokenize(names, BLOCK_SIZE)\n",
    "str_to_int = stoi(vocab)\n",
    "int_to_str = itos(vocab)\n",
    "\n",
    "\n",
    "# Splits\n",
    "b1 = math.floor(len(xs) * 0.8)\n",
    "b2 = math.floor(len(xs) * 0.9)\n",
    "x_train = xs[:b1]\n",
    "y_train = ys[:b1]\n",
    "x_test = xs[b1:b2]\n",
    "y_test = ys[b1:b2]\n",
    "x_dev = xs[b2:]\n",
    "y_dev = ys[b2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "603b628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize emb matrix\n",
    "emb = np.random.randn(len(vocab), FEATURES)\n",
    "\n",
    "# Initialize model\n",
    "name_net = MLP(size, tanh, dtanh, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93f57084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss: 0.1278\n",
      "step 1000 | loss: 0.0972\n",
      "step 2000 | loss: 0.0834\n",
      "step 3000 | loss: 0.0820\n",
      "step 4000 | loss: 0.0791\n",
      "step 5000 | loss: 0.0912\n",
      "step 6000 | loss: 0.0690\n",
      "step 7000 | loss: 0.0698\n",
      "step 8000 | loss: 0.0836\n",
      "step 9000 | loss: 0.0677\n",
      "step 10000 | loss: 0.0751\n",
      "step 11000 | loss: 0.1011\n",
      "step 12000 | loss: 0.0713\n",
      "step 13000 | loss: 0.0718\n",
      "step 14000 | loss: 0.0707\n",
      "step 15000 | loss: 0.0732\n",
      "step 16000 | loss: 0.0856\n",
      "step 17000 | loss: 0.0677\n",
      "step 18000 | loss: 0.0660\n",
      "step 19000 | loss: 0.0912\n",
      "step 20000 | loss: 0.0959\n",
      "step 21000 | loss: 0.0699\n",
      "step 22000 | loss: 0.0642\n",
      "step 23000 | loss: 0.0755\n",
      "step 24000 | loss: 0.0786\n",
      "step 25000 | loss: 0.0951\n",
      "step 26000 | loss: 0.0747\n",
      "step 27000 | loss: 0.0754\n",
      "step 28000 | loss: 0.0784\n",
      "step 29000 | loss: 0.0788\n",
      "step 30000 | loss: 0.0847\n",
      "step 31000 | loss: 0.0789\n",
      "step 32000 | loss: 0.0627\n",
      "step 33000 | loss: 0.0940\n",
      "step 34000 | loss: 0.0733\n",
      "step 35000 | loss: 0.0856\n",
      "step 36000 | loss: 0.0783\n",
      "step 37000 | loss: 0.0660\n",
      "step 38000 | loss: 0.0817\n",
      "step 39000 | loss: 0.0841\n",
      "step 40000 | loss: 0.0852\n",
      "step 41000 | loss: 0.0760\n",
      "step 42000 | loss: 0.0793\n",
      "step 43000 | loss: 0.0652\n",
      "step 44000 | loss: 0.0626\n",
      "step 45000 | loss: 0.0714\n",
      "step 46000 | loss: 0.0778\n",
      "step 47000 | loss: 0.0819\n",
      "step 48000 | loss: 0.0741\n",
      "step 49000 | loss: 0.0763\n",
      "step 50000 | loss: 0.0845\n",
      "step 51000 | loss: 0.0829\n",
      "step 52000 | loss: 0.0801\n",
      "step 53000 | loss: 0.0745\n",
      "step 54000 | loss: 0.0705\n",
      "step 55000 | loss: 0.0946\n",
      "step 56000 | loss: 0.0804\n",
      "step 57000 | loss: 0.0761\n",
      "step 58000 | loss: 0.0774\n",
      "step 59000 | loss: 0.0872\n",
      "step 60000 | loss: 0.0795\n",
      "step 61000 | loss: 0.0863\n",
      "step 62000 | loss: 0.0739\n",
      "step 63000 | loss: 0.0657\n",
      "step 64000 | loss: 0.0824\n",
      "step 65000 | loss: 0.0626\n",
      "step 66000 | loss: 0.0616\n",
      "step 67000 | loss: 0.0689\n",
      "step 68000 | loss: 0.0918\n",
      "step 69000 | loss: 0.0751\n",
      "step 70000 | loss: 0.0899\n",
      "step 71000 | loss: 0.0828\n",
      "step 72000 | loss: 0.0752\n",
      "step 73000 | loss: 0.0799\n",
      "step 74000 | loss: 0.0796\n",
      "step 75000 | loss: 0.0700\n",
      "step 76000 | loss: 0.0700\n",
      "step 77000 | loss: 0.0842\n",
      "step 78000 | loss: 0.0899\n",
      "step 79000 | loss: 0.0936\n",
      "step 80000 | loss: 0.0740\n",
      "step 81000 | loss: 0.0695\n",
      "step 82000 | loss: 0.0660\n",
      "step 83000 | loss: 0.0724\n",
      "step 84000 | loss: 0.0691\n",
      "step 85000 | loss: 0.0805\n",
      "step 86000 | loss: 0.0908\n",
      "step 87000 | loss: 0.0623\n",
      "step 88000 | loss: 0.0730\n",
      "step 89000 | loss: 0.0628\n",
      "step 90000 | loss: 0.0796\n",
      "step 91000 | loss: 0.0760\n",
      "step 92000 | loss: 0.0728\n",
      "step 93000 | loss: 0.0850\n",
      "step 94000 | loss: 0.0842\n",
      "step 95000 | loss: 0.0817\n",
      "step 96000 | loss: 0.0813\n",
      "step 97000 | loss: 0.0698\n",
      "step 98000 | loss: 0.0795\n",
      "step 99000 | loss: 0.0708\n",
      "step 100000 | loss: 0.0753\n",
      "step 101000 | loss: 0.0618\n",
      "step 102000 | loss: 0.0559\n",
      "step 103000 | loss: 0.0600\n",
      "step 104000 | loss: 0.0657\n",
      "step 105000 | loss: 0.0884\n",
      "step 106000 | loss: 0.0706\n",
      "step 107000 | loss: 0.0754\n",
      "step 108000 | loss: 0.0701\n",
      "step 109000 | loss: 0.0592\n",
      "step 110000 | loss: 0.0607\n",
      "step 111000 | loss: 0.0636\n",
      "step 112000 | loss: 0.0535\n",
      "step 113000 | loss: 0.0618\n",
      "step 114000 | loss: 0.0679\n",
      "step 115000 | loss: 0.0572\n",
      "step 116000 | loss: 0.0751\n",
      "step 117000 | loss: 0.0753\n",
      "step 118000 | loss: 0.0587\n",
      "step 119000 | loss: 0.0580\n",
      "step 120000 | loss: 0.0691\n",
      "step 121000 | loss: 0.0667\n",
      "step 122000 | loss: 0.0561\n",
      "step 123000 | loss: 0.0643\n",
      "step 124000 | loss: 0.0712\n",
      "step 125000 | loss: 0.0657\n",
      "step 126000 | loss: 0.0650\n",
      "step 127000 | loss: 0.0744\n",
      "step 128000 | loss: 0.0655\n",
      "step 129000 | loss: 0.0620\n",
      "step 130000 | loss: 0.0657\n",
      "step 131000 | loss: 0.0619\n",
      "step 132000 | loss: 0.0703\n",
      "step 133000 | loss: 0.0536\n",
      "step 134000 | loss: 0.0665\n",
      "step 135000 | loss: 0.0713\n",
      "step 136000 | loss: 0.0609\n",
      "step 137000 | loss: 0.0565\n",
      "step 138000 | loss: 0.0688\n",
      "step 139000 | loss: 0.0648\n",
      "step 140000 | loss: 0.0643\n",
      "step 141000 | loss: 0.0558\n",
      "step 142000 | loss: 0.0472\n",
      "step 143000 | loss: 0.0547\n",
      "step 144000 | loss: 0.0556\n",
      "step 145000 | loss: 0.0689\n",
      "step 146000 | loss: 0.0587\n",
      "step 147000 | loss: 0.0481\n",
      "step 148000 | loss: 0.0636\n",
      "step 149000 | loss: 0.0619\n",
      "step 150000 | loss: 0.0653\n",
      "step 151000 | loss: 0.0543\n",
      "step 152000 | loss: 0.0661\n",
      "step 153000 | loss: 0.0647\n",
      "step 154000 | loss: 0.0633\n",
      "step 155000 | loss: 0.0707\n",
      "step 156000 | loss: 0.0608\n",
      "step 157000 | loss: 0.0651\n",
      "step 158000 | loss: 0.0533\n",
      "step 159000 | loss: 0.0731\n",
      "step 160000 | loss: 0.0659\n",
      "step 161000 | loss: 0.0589\n",
      "step 162000 | loss: 0.0685\n",
      "step 163000 | loss: 0.0554\n",
      "step 164000 | loss: 0.0579\n",
      "step 165000 | loss: 0.0657\n",
      "step 166000 | loss: 0.0704\n",
      "step 167000 | loss: 0.0686\n",
      "step 168000 | loss: 0.0720\n",
      "step 169000 | loss: 0.0721\n",
      "step 170000 | loss: 0.0541\n",
      "step 171000 | loss: 0.0628\n",
      "step 172000 | loss: 0.0677\n",
      "step 173000 | loss: 0.0579\n",
      "step 174000 | loss: 0.0544\n",
      "step 175000 | loss: 0.0542\n",
      "step 176000 | loss: 0.0652\n",
      "step 177000 | loss: 0.0568\n",
      "step 178000 | loss: 0.0564\n",
      "step 179000 | loss: 0.0572\n",
      "step 180000 | loss: 0.0612\n",
      "step 181000 | loss: 0.0724\n",
      "step 182000 | loss: 0.0603\n",
      "step 183000 | loss: 0.0590\n",
      "step 184000 | loss: 0.0630\n",
      "step 185000 | loss: 0.0523\n",
      "step 186000 | loss: 0.0564\n",
      "step 187000 | loss: 0.0584\n",
      "step 188000 | loss: 0.0521\n",
      "step 189000 | loss: 0.0762\n",
      "step 190000 | loss: 0.0660\n",
      "step 191000 | loss: 0.0545\n",
      "step 192000 | loss: 0.0635\n",
      "step 193000 | loss: 0.0654\n",
      "step 194000 | loss: 0.0677\n",
      "step 195000 | loss: 0.0694\n",
      "step 196000 | loss: 0.0607\n",
      "step 197000 | loss: 0.0587\n",
      "step 198000 | loss: 0.0612\n",
      "step 199000 | loss: 0.0613\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "grad_descent(name_net, x_train, y_train, STEPS, BATCH_SIZE, LR, ALPHA)\n",
    "\n",
    "# Hyperparam Training\n",
    "# grad_descent(name_net, x_dev, y_dev, ITERS, EPOCHS, BATCH_SIZE, LR, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eac0c032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.106314837614564"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check entropy\n",
    "name_net.forward(x_train)\n",
    "preds = np.max(name_net.layers[-1].value, axis=1)\n",
    "float(cross_entropy(preds, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c55124da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nara\n",
      "alette\n",
      "kaine\n",
      "zelei\n",
      "carley\n",
      "kristyn\n",
      "alai\n",
      "patrin\n",
      "khyra\n",
      "hamada\n",
      "carly\n",
      "daron\n",
      "nalaya\n",
      "evalin\n",
      "aneima\n",
      "milea\n",
      "ilyn\n",
      "kesha\n",
      "anna\n",
      "avalee\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "for _ in range(20):\n",
    "    input = [str_to_int['.']] * BLOCK_SIZE # SEED\n",
    "    out = \"\"\n",
    "    while '.' not in out:\n",
    "        name_net.forward(np.array(input), temperature=TEMPERATURE)\n",
    "        probs = name_net.layers[-1].value\n",
    "        i = np.random.choice(VOCAB_SIZE, p=probs.flatten())\n",
    "        out += int_to_str[i]\n",
    "        input = input[1:] + [i]\n",
    "    print(out[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NameNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
